{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Article Page Info MediaWiki API Example\n",
    "This example illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). This example shows how to request summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example relies on some constants that help make the code a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<es2@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "\n",
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The function get_revID_for_articles takes in the location of a CSV which contains politician names and returns the output from a wikipedia API call as a dataframe. If the parameter full_dataframe is set to true, it'll return all information pulled from the wikipedia API, while if full_dataframe is set to False, it'll just return the article title and the last revision ID. The parameter remove_missing_rev_id will filter out articles that don't have a last revision ID if set to True or will keep them in with a Nan value if set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revID_for_articles(csv_loc, \n",
    "                           full_dataframe = False, \n",
    "                           remove_missing_rev_id = True):\n",
    "    \"\"\"\n",
    "    Given the location of wikipedia article titles, output the results from a Wikipedia API call\n",
    "    Parameters:\n",
    "        - csv_loc: File location of CSV with article titles.\n",
    "        - full_dataframe: Determines whether the full results from the API call (True) are returned \n",
    "                            or whether it's just the last revision ID (False) \n",
    "        - remove_missing_rev_id: Determines whether to filter out articles with no revision ID (True) or leave them in as Nan (False)\n",
    "    Returns a dataframe of results\n",
    "    \"\"\"\n",
    "    # load the data in\n",
    "    politicians_by_country = pd.read_csv(f\"data/{csv_loc}.csv\")\n",
    "\n",
    "    if politicians_by_country.isna().any().any():  # Check if there are any NA values in the DataFrame\n",
    "        na_rows = politicians_by_country[politicians_by_country.isna().any(axis=1)]  # Get rows with any NA values\n",
    "        num_na_rows = len(na_rows)  # Count those rows\n",
    "        print(f\"There are {na_rows} missing rows. They are listed below:\")\n",
    "        print(na_rows)\n",
    "        print(\"They've been omitted\")\n",
    "    \n",
    "    politician_list = politicians_by_country.dropna()[\"name\"].tolist()\n",
    "    unique_politician_list = list(set(politician_list))\n",
    "    num_duplicates = len(politician_list) - len(unique_politician_list)\n",
    "    print(f\"There are {num_duplicates} duplicate politicians. This does not affect the revision ID collection\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Process articles in batches\n",
    "    for i in range(0, len(unique_politician_list), 50):  # Example: process 50 articles at a time\n",
    "        batch_titles = \"|\".join(unique_politician_list[i:i+50])  # Join article titles with '|'\n",
    "        article_info = request_pageinfo_per_article(batch_titles)  # Call the function with the batched titles\n",
    "        \n",
    "        for keys, values in article_info[\"query\"][\"pages\"].items():\n",
    "            row = {}\n",
    "            for column_name in values:\n",
    "                row[column_name] = values[column_name]\n",
    "            \n",
    "            rows.append(row)  # Append the entire row dictionary \n",
    "\n",
    "    # Create a DataFrame from the list of rows\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if remove_missing_rev_id:\n",
    "        removed_rows = df[df['lastrevid'].isna()]\n",
    "        initial_row_count = df.shape[0]\n",
    "        df.dropna(subset=['lastrevid'], inplace=True)\n",
    "        final_row_count = df.shape[0]\n",
    "        removed_row_count = initial_row_count - final_row_count\n",
    "        pct_missing = removed_row_count / len(unique_politician_list)\n",
    "        print(\"Number of rows removed due to missing last_revisionID:\", removed_row_count)\n",
    "        print(\"Percentage of politicians missing last_revisionID:\", pct_missing)\n",
    "        if not removed_row_count == 0:\n",
    "            print(\"Politicians removed:\", removed_rows[\"title\"].values.tolist())\n",
    "\n",
    "    \n",
    "    # Convert lastrevid to integer\n",
    "    df[\"lastrevid\"] = df[\"lastrevid\"].astype(int)\n",
    "\n",
    "    if full_dataframe:\n",
    "        return df  # Return the DataFrame\n",
    "    else:\n",
    "        return df[[\"title\", \"lastrevid\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function call below inputs the location of the CSV with the politician article names and specifies that we only want the article title and the last revision ID. The idea is that a research can adjust this call to switch to the full dataframe if they want with a simple adjustment of the call below. We also filter out missing revision ID's. \n",
    "\n",
    "The missing politicians are also listed below and the % missing is given. Because there are so few politicians that are missing revision IDs we aren't worried and we will omit them from our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44 duplicate politicians. This does not affect the revision ID collection\n",
      "Number of rows removed due to missing last_revisionID: 8\n",
      "Percentage of politicians missing last_revisionID: 0.0011250175783996624\n",
      "Politicians removed: [\"Segun ''Aeroland'' Adewale\", 'Tomás Pimentel', 'Richard Sumah', 'Mehrali Gasimov', 'Bashir Bililiqo', 'Kyaw Myint', 'André Ngongang Ouandji', 'Barbara Eibinger-Miedl']\n"
     ]
    }
   ],
   "source": [
    "# full_dataframes returns the full dataframe with all columns\n",
    "# remove missing_rev_id takes out article titles that don't have revID\n",
    "politician_revid = get_revID_for_articles(\"politicians_by_country_AUG.2024\", full_dataframe=False, remove_missing_rev_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below just saves the dataframe as a CSV to the location of choice. A researcher could adjust this depending on their desired save location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the dataframe - adjust based on where you want the final dataframe\n",
    "politician_revid.to_csv(\"results/politician_revid.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
